{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c12e76-e7b7-48d9-affd-a3cb3c2d318c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_pickle('shared/Project-3_NYC_311_Calls.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a3667c-2434-4a9e-831d-aa7b82cff14f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of daily complaints received in 2022: 8684.320547945206\n"
     ]
    }
   ],
   "source": [
    "#Q1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the pickle file into a DataFrame\n",
    "df = pd.read_pickle('shared/Project-3_NYC_311_Calls.pkl')\n",
    "\n",
    "# Make the index a proper DatetimeIndex\n",
    "df = df.set_index(pd.DatetimeIndex(df['Created Date']))\n",
    "\n",
    "# Delete the 'Created Date' column\n",
    "del df['Created Date']\n",
    "\n",
    "# Resample by day and count the number of unique keys (complaints) per day\n",
    "daily_complaints = df['Unique Key'].resample('D').count()\n",
    "\n",
    "# Filter data for the year 2022\n",
    "daily_complaints_2022 = daily_complaints['2022']\n",
    "\n",
    "# Calculate the average number of daily complaints in 2022\n",
    "average_daily_complaints_2022 = daily_complaints_2022.mean()\n",
    "\n",
    "print(\"Average number of daily complaints received in 2022:\", average_daily_complaints_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d34eb4-7f3a-45b7-9209-687597692f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date with maximum number of calls: 2020-08-04 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the pickle file into a DataFrame\n",
    "df = pd.read_pickle('shared/Project-3_NYC_311_Calls.pkl')\n",
    "\n",
    "# Make the index a proper DatetimeIndex\n",
    "df = df.set_index(pd.DatetimeIndex(df['Created Date']))\n",
    "\n",
    "# Delete the 'Created Date' column\n",
    "del df['Created Date']\n",
    "\n",
    "# Resample by day and count the number of unique keys (complaints) per day\n",
    "daily_complaints = df['Unique Key'].resample('D').count()\n",
    "# Find the single date with the maximum number of calls\n",
    "max_calls_date = df['Unique Key'].resample('D').count().idxmax()\n",
    "\n",
    "print(f\"Date with maximum number of calls: {max_calls_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdbb1d1-4f9f-4422-b6c6-a0bf46aeb0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 2020-08-04, the most important complaint type was: Damaged Tree\n"
     ]
    }
   ],
   "source": [
    "#Q3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the pickle file\n",
    "df = pd.read_pickle('shared/Project-3_NYC_311_Calls.pkl')\n",
    "\n",
    "# Set 'Created Date' as the index\n",
    "df = df.set_index(pd.DatetimeIndex(df['Created Date']))\n",
    "del df['Created Date']\n",
    "\n",
    "# Filter data for 2020-08-04 and find the most common complaint type\n",
    "most_common_complaint = df[df.index.date == pd.to_datetime('2020-08-04').date()]['Complaint Type'].mode().iloc[0]\n",
    "\n",
    "# Print the result\n",
    "print(\"On 2020-08-04, the most important complaint type was:\", most_common_complaint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1c9af4-a955-456a-8488-7f729b929453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quietest month historically is: Aug\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "import pandas as pd\n",
    "\n",
    "# Read the pickle file into a DataFrame\n",
    "df = pd.read_pickle('shared/Project-3_NYC_311_Calls.pkl')\n",
    "\n",
    "# Convert 'Created Date' to datetime\n",
    "df['Created Date'] = pd.to_datetime(df['Created Date'])\n",
    "\n",
    "# Extract month and year from the 'Created Date' column\n",
    "df['Month'] = df['Created Date'].dt.month\n",
    "df['Year'] = df['Created Date'].dt.year\n",
    "\n",
    "# Group by month and year, count unique calls, and find the quietest month\n",
    "quietest_month = df.groupby(['Year', 'Month'])['Unique Key'].count().idxmin()\n",
    "quietest_month_name = pd.to_datetime(f\"{quietest_month[0]}-{quietest_month[1]}-01\").strftime('%b')\n",
    "\n",
    "print(f\"The quietest month historically is: {quietest_month_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407f4d04-6713-4f16-a4f1-967a83d8ff95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the seasonal component on 2020-12-25 is: 183\n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Read the pickle file into a DataFrame\n",
    "df = pd.read_pickle('shared/Project-3_NYC_311_Calls.pkl')\n",
    "\n",
    "# Convert 'Created Date' to datetime and set it as the index\n",
    "df['Created Date'] = pd.to_datetime(df['Created Date'])\n",
    "df = df.set_index('Created Date')\n",
    "\n",
    "# Resample the time series to daily frequency using the Unique Key as a count\n",
    "daily_calls = df['Unique Key'].resample('D').count()\n",
    "\n",
    "# Perform ETS decomposition based on an additive model\n",
    "result = sm.tsa.seasonal_decompose(daily_calls, model='additive')\n",
    "\n",
    "# Extract the seasonal component for the specified date\n",
    "target_date = '2020-12-25'\n",
    "seasonal_component_on_date = result.seasonal.loc[target_date]\n",
    "\n",
    "print(\"The value of the seasonal component on {} is: {}\".format(target_date, round(seasonal_component_on_date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12439100-7ce1-42ac-89b4-618f604509ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autocorrelation with lag of 1 is: 0.7517059728398577\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the pickle file into a DataFrame\n",
    "df = pd.read_pickle('shared/Project-3_NYC_311_Calls.pkl')\n",
    "\n",
    "# Extract necessary columns and set the 'Created Date' as the index\n",
    "df = df[['Created Date', 'Unique Key']]\n",
    "df = df.set_index('Created Date')\n",
    "\n",
    "# Resample the time series to daily frequency\n",
    "daily_calls = df.resample('D').size()\n",
    "\n",
    "# Calculate autocorrelation with lag of 1\n",
    "autocorrelation_lag1 = daily_calls.autocorr(lag=1)\n",
    "\n",
    "print(f\"The autocorrelation with lag of 1 is: {autocorrelation_lag1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ac5095-d6aa-4cac-83ae-f1551f3affbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE on the test set is: 1246.901274959065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mggy8413/lib/python3.10/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['typ']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Q7\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Read the pickle file into a DataFrame\n",
    "df = pd.read_pickle('shared/Project-3_NYC_311_Calls.pkl')\n",
    "\n",
    "# Extract necessary columns and set the 'Created Date' as the index\n",
    "df = df[['Created Date', 'Unique Key']]\n",
    "df = df.set_index('Created Date')\n",
    "\n",
    "# Resample the time series to daily frequency\n",
    "daily_calls = df.resample('D').size().reset_index()\n",
    "daily_calls.columns = ['ds', 'y']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train = daily_calls[:-90]\n",
    "test = daily_calls[-90:]\n",
    "\n",
    "# Initialize the ARIMA model\n",
    "order = (1, 1, 1)  # You can adjust the order based on your data characteristics\n",
    "model = ARIMA(train['y'], order=order)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Generate predictions on the test set\n",
    "predictions = model_fit.predict(start=len(train), end=len(train) + len(test) - 1, typ='levels')\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(test['y'], predictions))\n",
    "\n",
    "print(f\"The RMSE on the test set is: {rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mggy8413]",
   "language": "python",
   "name": "conda-env-mggy8413-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
